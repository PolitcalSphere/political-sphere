import{_ as s,c as e,o as i,ae as l}from"./chunks/framework.CBTkueSR.js";const k=JSON.parse('{"title":"Local AI helpers (offline)","description":"","frontmatter":{},"headers":[],"relativePath":"ai-on-dev.md","filePath":"ai-on-dev.md"}'),n={name:"ai-on-dev.md"};function t(o,a,r,p,h,c){return i(),e("div",null,[...a[0]||(a[0]=[l(`<h1 id="local-ai-helpers-offline" tabindex="-1">Local AI helpers (offline) <a class="header-anchor" href="#local-ai-helpers-offline" aria-label="Permalink to &quot;Local AI helpers (offline)&quot;">​</a></h1><p>This section describes small offline helpers to generate Conventional Commit messages, PR descriptions and quick PR reviews using a local LLM runtime such as Ollama running a Llama 3 model.</p><p>Prerequisites</p><ul><li>Install Ollama locally if you want model-generated text: <a href="https://ollama.ai" target="_blank" rel="noreferrer">https://ollama.ai</a></li><li>Ensure <code>ollama</code> CLI is on your PATH.</li></ul><p>Scripts</p><ul><li><code>npm run ai:commit</code> — prints a Conventional Commit header plus a short 5-line PR body. Uses local Ollama <code>llama3</code> when available, otherwise falls back to a heuristic.</li><li><code>npm run ai:pr</code> — generates a PR title and 5-8 line PR description (summary, changes, testing steps, checklist).</li><li><code>npm run ai:review</code> — lightweight PR review checklist and suggested focus areas.</li></ul><p>Design</p><ul><li>All scripts are pure-node and do not call external cloud APIs. If <code>ollama</code> is present, they will use the local model for higher quality output. Otherwise they will use deterministic fallbacks.</li></ul><p>Usage</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># generate a commit message you can paste</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ai:commit</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># generate a PR description</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ai:pr</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># quick review notes for the current branch</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ai:review</span></span></code></pre></div><p>Security &amp; Privacy</p><ul><li>These scripts do not send any data to remote services by default. If you install Ollama and its models, those models run locally on your machine.</li></ul><h1 id="local-ai-helpers-offline-1" tabindex="-1">Local AI helpers (offline) <a class="header-anchor" href="#local-ai-helpers-offline-1" aria-label="Permalink to &quot;Local AI helpers (offline)&quot;">​</a></h1><p>This project includes tiny local scripts to help generate Conventional Commit messages and PR descriptions without calling any external cloud LLM APIs. They prefer local tools (like Ollama) if available, but always fall back to an offline heuristic so they work without network access.</p><p>Scripts</p><ul><li><code>dev/ai/commit-msg.mjs</code> — inspects staged files and prints a Conventional Commit suggestion (short header + body). Use it like:</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># preview suggestion</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">node</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dev/ai/commit-msg.mjs</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># create commit with suggested message (bash)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> add</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> .</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> commit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -F</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &lt;(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">node</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dev/ai/commit-msg.mjs)</span></span></code></pre></div><ul><li><code>dev/ai/review-pr.mjs</code> — generates a short PR description from recent commits:</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">node</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dev/ai/review-pr.mjs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> PR_BODY.md</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># then paste PR_BODY.md into the PR description</span></span></code></pre></div><p>Local LLM support</p><ul><li>If you run a local LLM runner like <code>ollama</code> with <code>llama3</code> available, these scripts can be extended to pipe prompts to the local model. Right now they intentionally avoid remote APIs to keep development offline-friendly.</li></ul><p>Why this is useful</p><ul><li>Speeds up writing good commit messages and PR descriptions.</li><li>Keeps all processing local and deterministic.</li></ul><p>If you&#39;d like, I can extend these scripts to call <code>ollama</code> when detected and include nicer natural-language summaries from the model. Say <code>yes</code> to enable that enhancement.</p><h1 id="ai-on-dev-local-ai-helpers-for-political-sphere" tabindex="-1">AI on Dev: Local AI Helpers for Political Sphere <a class="header-anchor" href="#ai-on-dev-local-ai-helpers-for-political-sphere" aria-label="Permalink to &quot;AI on Dev: Local AI Helpers for Political Sphere&quot;">​</a></h1><p>This document explains how to use the local AI-powered development tools in Political Sphere. All AI features run locally using Ollama and do not require any cloud APIs or paid services.</p><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="header-anchor" href="#prerequisites" aria-label="Permalink to &quot;Prerequisites&quot;">​</a></h2><ol><li>Install Ollama: <a href="https://ollama.ai/" target="_blank" rel="noreferrer">https://ollama.ai/</a></li><li>Pull the Llama 3 model: <code>ollama pull llama3</code></li></ol><h2 id="available-ai-scripts" tabindex="-1">Available AI Scripts <a class="header-anchor" href="#available-ai-scripts" aria-label="Permalink to &quot;Available AI Scripts&quot;">​</a></h2><h3 id="commit-message-generator-npm-run-ai-commit" tabindex="-1">Commit Message Generator (<code>npm run ai:commit</code>) <a class="header-anchor" href="#commit-message-generator-npm-run-ai-commit" aria-label="Permalink to &quot;Commit Message Generator (\`npm run ai:commit\`)&quot;">​</a></h3><p>Generates Conventional Commit messages from your staged changes.</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Stage your changes</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> add</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> .</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Generate commit message</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ai:commit</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Copy the output and commit</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> commit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;feat: add new feature</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Generated description...&quot;</span></span></code></pre></div><h3 id="pr-review-helper-npm-run-ai-review" tabindex="-1">PR Review Helper (<code>npm run ai:review</code>) <a class="header-anchor" href="#pr-review-helper-npm-run-ai-review" aria-label="Permalink to &quot;PR Review Helper (\`npm run ai:review\`)&quot;">​</a></h3><p>Performs a basic AI-powered code review on your changes. Falls back to linting if Ollama is not available.</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># After making changes</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ai:review</span></span></code></pre></div><h2 id="how-it-works" tabindex="-1">How It Works <a class="header-anchor" href="#how-it-works" aria-label="Permalink to &quot;How It Works&quot;">​</a></h2><ul><li><strong>Local Only</strong>: All processing happens on your machine using Ollama</li><li><strong>No Data Sent</strong>: Code diffs are processed locally, never transmitted</li><li><strong>Fallback Mode</strong>: If Ollama isn&#39;t running, scripts fall back to standard linting</li><li><strong>Zero Cost</strong>: No API calls or subscription fees</li></ul><h2 id="installation" tabindex="-1">Installation <a class="header-anchor" href="#installation" aria-label="Permalink to &quot;Installation&quot;">​</a></h2><p>Ollama is optional for development. If not installed, the scripts will gracefully fall back to rule-based checks.</p><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">​</a></h2><ul><li><strong>Ollama not found</strong>: Install Ollama and run <code>ollama pull llama3</code></li><li><strong>Model not available</strong>: Ensure Llama 3 is pulled: <code>ollama pull llama3</code></li><li><strong>Slow performance</strong>: AI processing may take time depending on your hardware</li></ul><h2 id="security-note" tabindex="-1">Security Note <a class="header-anchor" href="#security-note" aria-label="Permalink to &quot;Security Note&quot;">​</a></h2><p>All AI processing is local. No code or diffs are sent to external services.</p>`,43)])])}const m=s(n,[["render",t]]);export{k as __pageData,m as default};
